# Goal of this package

Adobe Experience Platform (AEP) is the data layer under the Adobe campaigning stack. It has an API first design which enables easy manipulation through code. For full API reference see: https://www.adobe.io/apis/experienceplatform/home/api-reference.html

This package provides a python wrapper around the AEP API to make manipulation of AEP artifacts in Python easy.
The main use-case is to enable the KPN decisioning framework in AEP.

# How to use the package
For now, clone this repo and pip install it. This package will be published on KPN pypi soon.

The code expects a aep_config.yaml file in the folder you run your code from. This yaml contains data to set up authentication to AEP. Private fields (api key, client secret etc.) can be passed through environment variables by specifying them as `!ENV ${some_var}` in your aep_config.yaml. 

The private_key.pem file should also be passed as an environment variable. To do this, read the .pem file in python using:
```
priv_key_file = open(priv_key_filename, "rb")

priv_key = priv_key_file.read()
```
Pass the string  stored under priv_key variable (without "" marks, but with all escape characters) as an environment variable. 

# Design pattern
## AEP API Design
AEP APIs have a RESTful design and have a hierarchical structure. The endpoints are grouped in collections. A collection represents some logical topic of endpoints that belong together.
Some examples are:
+ Sensei machine learning (collection for machine learning)
    + engine (endpoint)
    + mlinstance (endpoint)
    + experiment (endpoint)
    + experimentrun (endpoint)
    + ...
+ Segmentation service (collection for segmentation)
    + export jobs (endpoint)
    + previews (endpoint)
    + segment definitions (endpoint)
    + ...

Every endpoint exposes `GET`, `POST`, `DELETE` and `PATCH` methods. In general:
+ `POST` creates a new resource on AEP by providing a JSON that describes the new resource. AEP returns a response containing the definition of the created resource with a unique id generated by AEP.
+ `GET` on the endpoint appended by the resource id retrieves the definition of an existing resource.
+ `DELETE` on the endpoint appended by the resource id deletes an existing resource.
+ `PATCH` on the endpoint updates the resource definition.

 ## Package design

This package wraps the AEP APIs by representing each endpoint by a Python object `AEPObject`.  Multiple endpoints are bundled under a collection `AEPCollection`. For implemented endpoints and collections see known_endpoints.yaml in paaw folder.

To set up the correct headers first an `AEP` object is created. On the background this retrieves the correct headers and sets up a Python requests session to handle the API calls. This object has all the available collections as attributes via `AEP.{collection_name}`.

To create an object in some collection we call `aep.{collection_name}.create_{objectname}` and pass a path to a .yaml file that describes the object. This results in a `POST` request to AEP to create the resource, and returns a Python `AEPObject` that represents the created resource. The .yaml file can contain placeholders that are filled during parsing. Variables with format `!ENV ${some_var}` are retrieve from environment variables. Variables with format `!ARG ${some_var}` are retrieved from the passed arg_replacements.

To retrieve an existing object in some collection we call `aep.{collection_name}.get_{objectname}` and pass the unique id of the resource. This makes a `GET` request to AEP and returns a Python `AEPObject` that represents the existing resource.

On an existing `AEPObject` we can call `.delete()`. This makes a `DELETE` request to AEP. If this is successful the `id` of the `AEPObject` is set to `None` to signify the successful deletion.

Some resources are created as a sub-resource to other resource. Rule of thumb is that when we have a hierarchy (flowrun belonging to flow, experimentrun belonging to experiment) the creation and retrieval of the bottom object goes via the top object instead of the collection. For example
```python 
experimentrun = experiment.start_experimentrun(path_to_config)
```
Some job-like resource also have extra endpoints to retrieve the status. In this case, this is implemented as a method of the `AEPObject` as well. For example
```python
experimentrun.poll_status()
```

Below are two examples that illustrate common patterns

### Create a dataset (basic creation through yaml files)
First we create the aep object to set up the headers.
```python
aep = AEP('aep_config.yaml')
```
The yaml contains variables required to make requests. `client_secret` and `api_key` are passed through environment variables. Note that which sandbox to deploy to is also defined here.
TODO: how to safely pass privatekey file from Jenkins?
```yaml
Server:
  ims_host: ims-na1.adobelogin.com
  ims_endpoint_jwt: /ims/exchange/jwt

# All the Enterprise values can be obtained from the Adobe IO Integration
Enterprise:
  api_key: !ENV ${AEP_API_KEY}
  org_id: BCC6148954F6271F0A4C98BC@AdobeOrg
  tech_acct: 835F575A5F240ED50A495E37@techacct.adobe.com
  client_secret: !ENV ${AEP_CLIENT_SECRET}
  priv_key_filename: private_key.pem

Platform:
  platform_gateway: https://platform.adobe.io
  ims_token: <ims_token>

Titles:
  sandbox_name: dev
``` 
Then, we are able to create a dataset. We will dynamically fill the dataset name by appending todays date.
```python
dataset_arg_repl = {'temp_id': str(datetime.now().date())}
dataset = aep.catalog_service.create_dataset(dataset_config_path, dataset_arg_repl)
```
In ```dataset_config_path``` we have the following .yaml:
```yaml
name: !ARG AN Profile Data Export ${temp_id} 
schemaRef:
  id: https://ns.adobe.com/xdm/context/profile__union
  contentType: application/vnd.adobe.xed+json;version=1
fileDescription:
  persisted: true
  containerFormat: parquet
  format: parquet
```
The resulting dataset object has two main attributes:`dataset.id`
> `'6093ab2fd921111948b5f57e'`

And `dataset.definition`:
> `??`

The dataset can be deleted as well
```python
dataset.delete()
```
This results in a delete request to AEP. If this was succesfull the `dataset.id` will be set to `None` to signify this Python object is not related to an object on AEP anymore.

We can also retrieve an existing dataset on AEP by:
```python
dataset = aep.catalog_service.get_dataset(dataset_id)
```

### Train a model for an existing experiment (retrieve existing objects and object methods)
Again, we start by creating the aep object
```python
aep = AEP('aep_config.yaml')
```

Then, we retrieve an existing experiment
```python
experiment_id = 'de1c6f20-80e2-4674-b4d5-03e1a0952b39'
experiment = aep.sensei.get_experiment(experiment_id)
```
The resulting expement object has two main attributes: `experiment.id`
> `'de1c6f20-80e2-4674-b4d5-03e1a0952b39'`

And `experiment.definition`:
> {'id': 'de1c6f20-80e2-4674-b4d5-03e1a0952b39',
 'sandboxId': 'e0216c19-9304-4223-a16c-199304922341',
 'name': 'Randomassigment Experiment',
 'mlInstanceId': '3247306c-2154-46f0-8512-0151876a137b',
 'created': '2021-04-15T16:12:34.893Z',
 'createdBy': {'userId': '835F575A5F240ED50A495E37@techacct.adobe.com'},
 'updated': '2021-04-15T16:12:34.893Z',
 'deprecated': False,
 'createdByService': False}

From this experiment we can retrieve the associated model by
```python
model = experiment.get_latest_model()
```
With the experiment object, a train config, and the id of latest model we can start a training run
```python
train_args = parse_config(os.path.join(config_path, 'model_parameters.yaml'),
                                        arg_replacements={'scoring_dataset_id': dataset.id,
                                                          'model_id': model.id})
experiment_run = experiment.start_experimentrun(train_config_path, train_args)
```
Note that we first retrieve all model parameters and dynamically fill some arguments. We then use the model parameters to correctly fill the final train config yaml.
Model parameters might look like:
```yaml
# copy these from the model parameters that is deployed and trained
decision_log_dataset_id: "60701ee33512c2194890887b"  
decison_stream_dataset_id: "60701ee0b62a6419494969f1"  
decision_schema_id: "cb1dc8877aebe7f282496cdcbdedbdc18cc2edc34d0a0023" 
stream_connection_id: "c35d37f64df1b6cc729289f32b6fe0fdbf2ef69f33c1a797081a9aa625abec12" 
stream_source_name: "NBA decision stream"
# These parameters are used during the score job
scoring_dataset_id: !ARG ${scoring_dataset_id} # retrieved when making the temp dataset
model_id : !ARG ${model_id} # retrieved by getting the latest model
mode: Score
```
Then we use the resulting key-value pairs after parsing to parse the train config, which might look like:
```yaml
{
"mode": !ARG "${mode}",
"tasks": [
{
  "name": "score",
    "parameters": [
      {
          "key": "scoringDataSetId",
          "value": !ARG "${scoring_dataset_id}"
      },
      {
          "key": "decision_log_dataset_id",
          "value": !ARG "${decision_log_dataset_id}"
      },
      {
          "key": "decison_stream_dataset_id",
          "value": !ARG "${decison_stream_dataset_id}"
      },
      {
          "key": "decision_attribute_schema_id",
          "value": !ARG "${decision_schema_id}"
      },
      {
          "key": "stream_connection_id",
          "value": !ARG "${stream_connection_id}"
      },
      {
          "key": "stream_source_name",
          "value": !ARG "${stream_source_name}"
      },
      {
          "key": "modelId",
          "value": !ARG "${model_id}"

      }
    ]
}]
}
```
This results in an experimentrun object that represents the training job. We can retrieve the status and url's to the logs by polling for the status:
```
status = experimentrun.poll_status()
```
resulting in `status`:
> `status response that contains urls to download stderr en stdout of running process`

# Currently implemented collections and endpoints
CatalogService:
+ Dataset
+ Batch

QueryService:
+ Query
+ ScheduledQuery

Segmentation Service:
+ ExportJob

Sensei ML Service:
+ Engine
+ MLInstance
+ Experiment
+ ExperimentRun
+ Model

# Processes that use this package
+ Models that are deployed and trained on AEP. First example is the random assignment decisioning model. https://git.kpn.org/projects/MDF/repos/kpn_cm_aep_decision_randomassignment
+ Query deployed on AEP to join past decisions and rewards. https://git.kpn.org/projects/MDF/repos/kpn_cm_aep_query_logger
+ Process that launches a score run for the current production decisioning model. https://git.kpn.org/projects/MDF/repos/kpn_cm_aep_decision_scoreprodmodel
+ Process to retrieve past decisions and map them to a reward (WIP) https://git.kpn.org/projects/MDF/repos/kpn_cm_aep_reward_conversioncounter/browse.

# Developer guide: extending this package.
We outline how to implement a new collection in the package.
## Step 1: add the collection uri to endpoint_parameters.yaml
All API calls in the same collection have a similar url.
The url starts with the platform gateway (https://platform.adobe.io) followed by a collection uri (/data/foundation/schemaregistry/ for schema registry for example.)
Add a new entry for the collection in endpoint_parameters.yaml. The key we will refer to as the collection name.
The value is the collection uri.

## Step 2: add the endpoints to known_endpoints.yaml
Under a collection, we have multiple endpoints.

First add a new top-level with the collection name (must match key of the uri in endpoint_parameters.yaml).
Under the collection, add a new level for each endpoint you want to implement. This we refer to as the endpoint name.

For every endpoint add the endpoint_url. The endpoint_parameters.yaml is parsed into this file when the AEP class is instantiated, so you may refer to fields in that yaml using a !ARG ${some_field} reference. 
Refer to the Adobe API documentation or the postman collections to find the url.

Add an extra_headers level. In the extra_headers level you can 
specify extra headers that are needed to make succesful calls to that endpoint.
You can generally find which extra headers are neccecary by looking at the API documentation or the postman collection.
## Step 3: Create the new collection object
Add a python file for the collection under models.
In this python file, create a subclass of AEPCollection, this represents the collection.
In the init of the superclass (AEPCollection), pass the collection name that you created in the endpoint_parameters.yaml.
Also add this collection object as an attribute of the AEP class (in aep.py in the init)
## Step 4: Create endpoint objects
Per endpoint you created in known_endpoints.yaml, create a subclass of AEPObject.
Set the name attribute of the class to the endpoint name you added to known_endpoints.yaml.
Set the id_find_func to some function that can retrieve the id from a get request.
The default is 
```python
id_find_func: lambda definition: definition['id']
```
because for most endpoints the id resides at the top level in the field 'id'.

## Intermezzo: making requests through the AEP object
All AEPObjects and AEPCollections have an _aep attribute through which under water the calls are made.
For create and get, its normally possible to just use the methods of these two classes.
For implementing specific behaviour of and AEPObject, you might have to make more fine-grained requests.
In general you can use the _aep object to make requests in two ways:
+ _aep.get, _aep.post, _aep.delete, ... Here you have to specify the path to the endpoint url as a . seperated string (collectionname.endpointname), the body, parameters and an optional url_suffix. The method takes care of building the complete url, parsing the body to json, and parsing the response back to a python dictionary. Also catches exceptions.
+ _aep.session.request. This is just the underlying requests object, so you can do everything that can be done by requests.
Use this for example if you have to do special trickery for the request to work, or if your response is not a json.
(see sensei.engine.create_from_config and dataaccess.datasetfile.get_file_under_pathname for examples). 
## Step 5: Fill the collection object with functions to create and get the endpoint objects
In most cases we want to create and get the endpoint objects we just created through the collection object.
For create, there are a few patterns:
### Create default endpoints
We can use the inherited behaviour from AEPObject and AEPCollection to create from config, by specifying inside the AEPCollection:
```python
def create_someendpoint(self, config_path: str, arg_replacements: Dict) -> SomeEndpointClass):
    return self._create_aepobject(SomeEndpointClass, config_path, arg_replacements)
```
on the background, this calls the _create_from_config classmethod of the AEPObject. By default this will first parse the config, and then make a post request. The url is determined by the collection name and endpoint name. The body will be the content of the config file. The response will be used to instantiate the AEPObject.
### Create endpoint with some non-default behaviour.
All endpoints implemented up to 3.3.3 are default, but who knows.... 
Here it really depends on the endpoint. Its probably best to still create via the collection 
through self._create_aepobject, but modify the creation behaviour via overwriting the _create_from_content classmethod of this specific AEPObject. 

For get, there is a default and non default implementation:
### Get default endpoint
We can use the inherited behaviour from AEPObject and AEPCollection get, by specifing inside the AEPCollection:
```python
def get_someendpoint(self, self, id: str) -> SomeEndpointClass:):
    return self._get_aepobject(SomeEndpointClass, id)
```
on the background, this uses a get request to the url defined by the collection name and endpont name.
The resulting response is used to instantiate the AEPObject.
### Get non-default endpoint.
For example, in schema and mixin creation, the id needs to be url encoded to have a valid get.
This is done by extending the get_someendpoint.
Sometimes we might want to create a get through a name instead of an id.
An example would be QueryService.get_list_scheduledqueries_by_name. We use aep._get functionality directly
instead of the inherited methods from the AEPCollection.

## Step 6: Implement specific methods for each endpoint.
This really depends on the endpoint. Look at Sensei.Experiment and QueryService.ScheduledQuery for some examples of AEPObjects with more extensive functionality

# Package improvements
+ Implement more collections, or missing endpoints in a collection. At this moment I'm only implementing the things I have a specific use-case for. 
+ Add _get_listof_aepobjects to AEPCollection. We have a recurring pattern of not retrieving a specific object, but a list of all objects.
+ Add tests. Need to research how to mock API responses.
+ Add more readable exception handling.



